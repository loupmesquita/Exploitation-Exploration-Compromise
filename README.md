# Exploitation-Exploration-Compromise

We consider a one dimensional world, with 8 possible positions, as defined in this folder.
An agent lives in this world, and can perform one of 3 actions at each time step : stay at its position, move right or move left.
<br>
Some rewards are placed in this world randomly, and are randomly updated perdiodically, at a fixed frequency. This means that a good agent should update its policy periodically as well and adapt to the new rewards. The agent knows about a reward in the world if its position has been on the same position as the reward, but each time the rewards are updated, the agents forgets all this knowledge, as implemented line 46 in simulation.py
<br>
simulation.py computes the statistical amount of reward obtained by the agent
and plots the evolution of this quantity in images/. As you can see in the images/
folder, the average accumulated reward with the default policy is around 16, with a
little bit of variance.
<br>
Multiple policies has been created to interact with the agent and aim to have the highest final average reward.