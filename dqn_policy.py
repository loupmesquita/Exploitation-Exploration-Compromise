import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

class DQN(nn.Module):
    def __init__(self, input_size, output_size):
        super(DQN, self).__init__()
        self.fc = nn.Linear(input_size, 128)
        self.relu = nn.ReLU()
        self.output_layer = nn.Linear(128, output_size)

    def forward(self, x):
        x = self.fc(x)
        x = self.relu(x)
        x = self.output_layer(x)
        return x

class DQNAgent:
    def __init__(self, input_size, output_size, epsilon=0.2, gamma=0.9):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.q_network = DQN(input_size, output_size).to(self.device)
        self.target_network = DQN(input_size, output_size).to(self.device)
        self.target_network.load_state_dict(self.q_network.state_dict())
        self.target_network.eval()
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)
        self.epsilon = epsilon
        self.gamma = gamma

    def select_action(self, state):
        with torch.no_grad():
            q_values = self.q_network(torch.tensor(state, dtype=torch.float32).to(self.device))
            if np.random.rand() < self.epsilon:
                # Explore: Choose a random action
                return np.random.choice(q_values.shape[0])
            else:
                # Exploit: Choose the action with the highest Q-value
                return torch.argmax(q_values).item()

    def update_q_values(self, state, action, reward, next_state):
        q_values = self.q_network(torch.tensor(state, dtype=torch.float32).to(self.device))
        next_q_values = self.target_network(torch.tensor(next_state, dtype=torch.float32).to(self.device))

        target_q = q_values.clone().detach()
        target_q[action] = reward + self.gamma * torch.max(next_q_values).item()

        loss = nn.MSELoss()(q_values, target_q)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def update_target_network(self):
        self.target_network.load_state_dict(self.q_network.state_dict())
