import numpy as np

class QLearningPolicy:
    def __init__(self, n_states, n_actions, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, alpha=0.5, gamma=0.9):
        self.n_states = n_states
        self.n_actions = n_actions
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay
        self.alpha = alpha
        self.gamma = gamma
        self.q_table = np.zeros((n_states, n_actions))

        # Add this mapping
        self.action_map = {"left": 0, "none": 1, "right": 2}

    def choose_action(self, state):
        if np.random.uniform(0, 1) < self.epsilon:
            action_index = np.random.choice(self.n_actions)  # Explore action space
        else:
            action_index = np.argmax(self.q_table[state])  # Exploit learned values

        # Decay epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

        # Map the action index to the corresponding action string
        action_map = {0: "left", 1: "none", 2: "right"}
        return action_map[action_index]

    def update_q_table(self, state, action, reward, next_state):
        old_value = self.q_table[state, self.action_map[action]]  # Use the mapping here
        future_optimal_value = np.max(self.q_table[next_state])
        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * future_optimal_value)
        self.q_table[state, self.action_map[action]] = new_value  # And here